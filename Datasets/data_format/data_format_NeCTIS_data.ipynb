{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657f29e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76320d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir '.\\NeCTIS Model Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c4bca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import fontstyle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8404bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_to_clean(line):\n",
    "    line = re.sub('( {1,})',' ',line)\n",
    "    line = re.sub('-$','',line) #to remove the - in the end\n",
    "    line = re.sub('<','',line)\n",
    "    line = re.sub('-',' ',line)\n",
    "    line = re.sub('(>\\w+)','',line)\n",
    "    line = re.sub(' $','',line) #to remove the space in the end\n",
    "    line = re.sub('^ ','',line) #to remove the space in the beginning\n",
    "    line = re.sub('( {1,})',' ',line)\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7769642",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Conversion(infile,outfile):\n",
    "    df = pd.read_csv(infile)\n",
    "    # #Uncomment df_new files to store data in dataframes\n",
    "    with open(outfile,'w') as w:\n",
    "        lines = df['Clean']    #Clean line\n",
    "        if 'Finegrain' in outfile:\n",
    "            raw_lines = df['Raw_Tagged']    #raw line for fine grain data\n",
    "        elif 'Coarse' in outfile:\n",
    "            raw_lines = df['Coarse_tag']    #raw line for Coarse data\n",
    "        # df_new = pd.DataFrame(columns= ['Word Index','Word','Comp Length','_','Related word index','Relation Tag'])\n",
    "        z = 0\n",
    "        for k in range(df.shape[0]):\n",
    "            print(k+1)\n",
    "            raw_line = raw_lines[k].strip()\n",
    "            line = lines[k]\n",
    "            comps_and_words = raw_line.strip().split() #space separated raw line gives list of compounds and individual words\n",
    "            clean_tokens = line.strip().split() #space separated clean line gives clean tokens\n",
    "            outmost_comp_list = [token for token in comps_and_words if '<' in token] #identifying compounds from comps_and_words\n",
    "            c = 0\n",
    "            d = 0\n",
    "#             print(raw_line)\n",
    "    #         w.write(raw_line+'\\n'*2)\n",
    "            i = 0\n",
    "    #         df_new.loc[z] = [raw_line,'','','','','']\n",
    "            z += 1\n",
    "            while i < len(clean_tokens):   ###traversing through sentence tokens\n",
    "                if c< len(outmost_comp_list):\n",
    "                    outcomp = outmost_comp_list[c]\n",
    "                clean_outcomp = raw_to_clean(outcomp) ### clean tokens of the outermost/parent compound\n",
    "                token = comps_and_words[d] ### choosing token i.e., compound and word from comps_and_words list\n",
    "                word = clean_tokens[i]\n",
    "                if word == token: ###for individual words other than compounds\n",
    "#                     print('\\033[94m'+f'{i+1}\\t{word}\\tCompNo\\t_\\t{len(clean_tokens)+1}\\tNo_rel'+'\\033[0m')### Red line\n",
    "                    w.write(f'{i+1}\\t{word}\\tCompNo\\t_\\t{len(clean_tokens)+1}\\tNo_rel'+'\\n')\n",
    "    #                 df_new.loc[z] = [i+1,word,'CompNo','_',len(clean_tokens)+1,'No_rel']\n",
    "                    z += 1\n",
    "                    i += 1\n",
    "                    d += 1\n",
    "                else:\n",
    "                    rem_string = outcomp\n",
    "                    comp_len = len(clean_outcomp.split())\n",
    "                    for p in range(comp_len):\n",
    "                        subword = clean_outcomp.split()[p] ### getting p-th subword of the compound\n",
    "                        if p==comp_len-1:   ###for last subword of an out-most compound\n",
    "#                             print('\\033[92m'+f'{i+1}\\t{subword}\\tComp{comp_len}\\t_\\t{len(clean_tokens)+1}\\tComp_root'+'\\033[0m')### Green line\n",
    "                            w.write(f'{i+1}\\t{subword}\\tComp{comp_len}\\t_\\t{len(clean_tokens)+1}\\tComp_root'+'\\n')\n",
    "    #                         df_new.loc[z] = [i+1,subword,'Comp'+str(comp_len),'_',len(clean_tokens)+1,'Comp_root']\n",
    "                            z += 1\n",
    "                            i += 1\n",
    "                        else:    ### for remaining subwords in compound\n",
    "                            subword_end = re.search('[^>]'+subword,rem_string).end() #getting end of the subword\n",
    "                            # using ^> so that words like ta, di don't interfere with tags\n",
    "                            rem_string = rem_string[subword_end:] ### remaining string after the subword/token\n",
    "                            n = 0\n",
    "                            ind = 0\n",
    "                            p = 0\n",
    "                            if rem_string[0] == '>': ###if remaining string starts with tag >\\w+\n",
    "                                flag = 1\n",
    "                            else:                   \n",
    "                                flag = 0\n",
    "                            for ind in range(len(rem_string)):\n",
    "                                if rem_string[ind] == '<': ###adding 1 to n if encountered an open bracket\n",
    "                                    n -= 1\n",
    "                                elif rem_string[ind] == '>': ###subtracting 1 from n if encountered an open bracket\n",
    "                                    n += 1\n",
    "                                elif rem_string[ind] == '-': ###adding 1 if encountered hyphen\n",
    "                                    p += 1\n",
    "                                if n-flag == 0: ### setting p to 0 if a compound is completed i.e., n-flag==0\n",
    "                                    p = 0\n",
    "                                if rem_string[0] == '>': #case1: remaining string starts with tag => >\\w+\n",
    "                                    #subcase1: remaining string has no new compound/ has an immediate relation word i.e., -\\w+\n",
    "                                    if rem_string.count('<')==0 or len(re.findall('^>\\w+-\\w+',rem_string))>0:\n",
    "                                        st = ind+re.search('-\\w+>',rem_string).end()-1\n",
    "                                        temp = rem_string[st:]\n",
    "                                        tag = re.findall('>(\\w+)',temp)[0]\n",
    "                                        break\n",
    "                                    else:\n",
    "                                        if len(re.findall('^>\\w+>\\w+',rem_string))==0: #subcase2 counterpart\n",
    "                                            if n-flag == 1 and p>=0:\n",
    "                                                st = ind\n",
    "                                                temp = rem_string[st:]\n",
    "                                                tag = re.findall('>(\\w+)',temp)[0]\n",
    "                                                break\n",
    "                                        else: #subcase2: multiple tags closing\n",
    "                                            #subsubcase1: multiple tags closing after the relation word\n",
    "                                            if len(re.findall('-\\w+(?:>\\w+)+>(\\w+)',rem_string))>0:  \n",
    "                                                st = re.search('-\\w+(?:>\\w+)+>(\\w+)',rem_string).end()-1\n",
    "                                                tag = re.findall('-\\w+(?:>\\w+)+>(\\w+)',rem_string)[0]\n",
    "                                            else: #subsubcase2: normal tags after relation word\n",
    "                                                g = re.findall('^(?:>\\w+)*>\\w+',rem_string)[0].count('>')\n",
    "                                                if n-flag-g==1:\n",
    "                                                    st = ind\n",
    "                                                    temp = rem_string[st:]\n",
    "                                                    tag = re.findall('>(\\w+)',temp)[0]\n",
    "                                            break\n",
    "                                #remaining cases: remaining string starting with related word => -\\w+ (or) compound => -<\\w+-\\w+>\\w+\n",
    "                                else: \n",
    "                                    if n-flag == 1:\n",
    "                                        st = ind\n",
    "                                        temp = rem_string[st:]\n",
    "                                        tag = re.findall('>(\\w+)',temp)[0]\n",
    "                                        break\n",
    "                            pre_tag_string = rem_string[:st]\n",
    "                            num = len(raw_to_clean(pre_tag_string).split())\n",
    "#                             print('\\033[91m'+f'{i+1}\\t{subword}\\tComp{comp_len}\\t_\\t{i+num+1}\\t{tag}'+'\\033[0m')\n",
    "                            w.write(f'{i+1}\\t{subword}\\tComp{comp_len}\\t_\\t{i+num+1}\\t{tag}'+'\\n')\n",
    "    #                         df_new.loc[z] = [i+1,subword,'Comp'+str(comp_len),'_',i+num+1,tag]\n",
    "                            z += 1\n",
    "                            i += 1\n",
    "                    c += 1\n",
    "                    d += 1\n",
    "#             print(f'{i+1}\\tDUMMY\\tCompNo\\t_\\t{0}\\troot')\n",
    "            w.write(f'{i+1}\\tDUMMY\\tCompNo\\t_\\t{0}\\troot\\n')\n",
    "    #         df_new.loc[z]= [i+1,'DUMMY','CompNo','_',0,'root']\n",
    "            z += 2\n",
    "#             print('\\n')\n",
    "            w.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04dbb526",
   "metadata": {},
   "source": [
    "#### With Context files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3428f41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd \"With Context CSV files\"\n",
    "\n",
    "csv_files = ['./train.csv','./dev.csv','./test.csv','./outofDomain.csv']\n",
    "out_files_Coarse = ['../NeCTIS Model Data/With Context/Coarse/'+csv[2:-3]+'txt' for csv in csv_files]\n",
    "out_files_finegrain = ['../NeCTIS Model Data/With Context/Finegrain/'+csv[2:-3]+'txt' for csv in csv_files]\n",
    "outfiles = [out_files_Coarse,out_files_finegrain]\n",
    "for k in range(2):\n",
    "    i = 0\n",
    "    for csv in csv_files:\n",
    "        outfile = outfiles[k][i]\n",
    "        infile = csv\n",
    "        Conversion(infile,outfile)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc03222",
   "metadata": {},
   "source": [
    "#### No Context files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6e199d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd \"../No Context CSV files\"\n",
    "\n",
    "csv_files = ['./train.csv','./dev.csv','./test.csv','./outofDomain.csv']\n",
    "out_files_Coarse = ['../NeCTIS Model Data/Without Context/Coarse/'+csv[2:-3]+'txt' for csv in csv_files]\n",
    "out_files_finegrain = ['../NeCTIS Model Data/Without Context/Finegrain/'+csv[2:-3]+'txt' for csv in csv_files]\n",
    "outfiles = [out_files_Coarse,out_files_finegrain]\n",
    "for k in range(2):\n",
    "    i = 0\n",
    "    for csv in csv_files:\n",
    "        outfile = outfiles[k][i]\n",
    "        infile = csv\n",
    "        Conversion(infile,outfile)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca0a5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd \"../\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87428b39",
   "metadata": {},
   "source": [
    "#### The format1-refers to the morph data and format2-refers to the label data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68197d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format1(file,outfile):\n",
    "    with open(file) as f:\n",
    "        lines = f.readlines()\n",
    "    with open(outfile,'w') as o:\n",
    "        for line in lines:\n",
    "            if line!= '\\n':\n",
    "                lst = line.strip().split('\\t')\n",
    "                lstnew = lst[:3]+[lst[2]]+lst[4:]+[lst[2]]\n",
    "                line_new = '\\t'.join(lstnew)\n",
    "                o.write(line_new+'\\n')\n",
    "            else:\n",
    "                o.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fd7df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format2(file,outfile):\n",
    "    with open(file) as f:\n",
    "        lines = f.readlines()\n",
    "    with open(outfile,'w') as o:\n",
    "        for line in lines:\n",
    "            if line!= '\\n':\n",
    "                lst = line.strip().split('\\t')\n",
    "                lstnew = lst[:3]+[lst[2]]+lst[4:]+[lst[5]]\n",
    "                line_new = '\\t'.join(lstnew)\n",
    "                o.write(line_new+'\\n')\n",
    "            else:\n",
    "                o.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe5e4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '' #root directory of the NeCTIS model data to generate new format data \n",
    "lst = []\n",
    "for path, subdirs, files in os.walk(root):\n",
    "    for name in files:\n",
    "        lst.append(os.path.join(path, name))\n",
    "# lst = [x for x in lst if 'Domain' not in x] #to skip out of domain folder\n",
    "print('List is:\\n')\n",
    "for p in lst:\n",
    "    print(p)\n",
    "infiles = lst\n",
    "outfiles = [] #create a list of file paths for the NeCTIS model data new format\n",
    "for i in range(len(infiles)):\n",
    "    print(infiles[i])\n",
    "    print(outfiles[i])\n",
    "    format1(infiles[i],outfiles[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63564bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '' #root directory of the NeCTIS model data to generate new format data \n",
    "lst = []\n",
    "for path, subdirs, files in os.walk(root):\n",
    "    for name in files:\n",
    "        lst.append(os.path.join(path, name))\n",
    "# lst = [x for x in lst if 'Domain' not in x] #to skip out of domain folder\n",
    "print('List is:\\n')\n",
    "for p in lst:\n",
    "    print(p)\n",
    "infiles = lst\n",
    "outfiles = [] #create a list of file paths for the NeCTIS model data new format\n",
    "for i in range(len(infiles)):\n",
    "    print(infiles[i])\n",
    "    print(outfiles[i])\n",
    "    format2(infiles[i],outfiles[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
