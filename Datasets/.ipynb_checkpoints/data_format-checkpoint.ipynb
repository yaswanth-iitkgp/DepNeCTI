{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adc0ad63",
   "metadata": {},
   "source": [
    "#### Instructions:\n",
    "Put this notebook in the folder containing folders of your with context csv files and without context csv files. For generating our data in the formats of baselines. You can use this notebook as is. For your data, you can modify the code accordingly. You just need to format the CSV files in our format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf064e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0c2490",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5dc039c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file .\\Data already exists.\n"
     ]
    }
   ],
   "source": [
    "!mkdir \".\\Data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0d1bbf",
   "metadata": {},
   "source": [
    "#### Function for POS in Clean line for LexCP, BotCP, Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eabfc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def POS(bio_tag):\n",
    "    bio_tag_split = bio_tag.strip().split()\n",
    "    lst = []\n",
    "    for i in range(len(bio_tag_split)):\n",
    "        bio = bio_tag_split[i]\n",
    "        if bio == 'B-C':\n",
    "            if bio_tag_split[i-1] == 'I-C'and i!=0:\n",
    "                lst.append(z+1)\n",
    "            z = 0\n",
    "        elif bio == 'I-C':\n",
    "            z += 1\n",
    "        elif bio == 'O':\n",
    "            if bio_tag_split[i-1] == 'I-C' and i!=0:\n",
    "                lst.append(z+1)\n",
    "                lst.append(1)\n",
    "            else:\n",
    "                lst.append(1)\n",
    "    if bio_tag_split[-1] == 'I-C':\n",
    "        lst.append(z+1)\n",
    "    lst_final = []\n",
    "    for b in lst:\n",
    "        for x in range(b):\n",
    "            lst_final.append(b)\n",
    "    lst_final = ['No' if key==1 else key for key in lst_final]\n",
    "    lst_final = ['Comp'+str(key) for key in lst_final]\n",
    "    \n",
    "    return lst_final\n",
    "\n",
    "def Clean_POS(clean_line,biotag):\n",
    "    \n",
    "    clean_tokens = clean_line.strip().split()\n",
    "    pos_bio_list = POS_BIO(biotag).split()\n",
    "    \n",
    "    if len(clean_tokens)!= len(pos_bio_list):\n",
    "        print('ERROR!!!!')\n",
    "    clean_pos_list = []\n",
    "    for i in range(len(pos_bio_list)):\n",
    "        if pos_bio_list[i][0] == 'B':\n",
    "            num = re.findall('\\d+',pos_bio_list[i])[0]\n",
    "            cpos = 'C'+num+'<'+clean_tokens[i]\n",
    "            clean_pos_list.append(cpos)\n",
    "        elif pos_bio_list[i][0] == 'I' and i!= len(pos_bio_list)-1:\n",
    "            if pos_bio_list[i+1][0] == 'B' or pos_bio_list[i+1][0] == 'O':\n",
    "                num = re.findall('\\d+',pos_bio_list[i])[0]\n",
    "                cpos = clean_tokens[i]+'>'+'C'+num\n",
    "                clean_pos_list.append(cpos)\n",
    "            else:\n",
    "                cpos = clean_tokens[i]\n",
    "                clean_pos_list.append(cpos)\n",
    "        elif pos_bio_list[i][0] == 'I' and i == len(pos_bio_list)-1:\n",
    "            num = re.findall('\\d+',pos_bio_list[i])[0]\n",
    "            cpos = clean_tokens[i]+'>'+'C'+num\n",
    "            clean_pos_list.append(cpos)\n",
    "        else:\n",
    "            cpos = clean_tokens[i]\n",
    "            clean_pos_list.append(cpos)\n",
    "    \n",
    "    clean_pos_line = ' '.join(clean_pos_list)\n",
    "    \n",
    "    return clean_pos_line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d5edce",
   "metadata": {},
   "source": [
    "#### Function for PO-TreeCRFs(CP) Formatting With POS in biotag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f32fab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def POS_BIO(bio_tag):\n",
    "    \n",
    "    bio_tag_split = bio_tag.strip().split()\n",
    "    lst = []\n",
    "    for i in range(len(bio_tag_split)):\n",
    "        bio = bio_tag_split[i]\n",
    "        if bio == 'B-C':\n",
    "            if bio_tag_split[i-1] == 'I-C'and i!=0:\n",
    "                lst.append(z+1)\n",
    "            z = 0\n",
    "        elif bio == 'I-C':\n",
    "            z += 1\n",
    "        elif bio == 'O':\n",
    "            if bio_tag_split[i-1] == 'I-C' and i!=0:\n",
    "                lst.append(z+1)\n",
    "                lst.append(1)\n",
    "            else:\n",
    "                lst.append(1)\n",
    "    if bio_tag_split[-1] == 'I-C':\n",
    "        lst.append(z+1)\n",
    "    lst_final = []\n",
    "    for b in lst:\n",
    "        for x in range(b):\n",
    "            lst_final.append(b)\n",
    "    \n",
    "    lst_final = ['No' if key==1 else key for key in lst_final]\n",
    "    lst_final = ['Comp'+str(key) for key in lst_final]\n",
    "    \n",
    "    if len(lst_final)!= len(bio_tag_split):\n",
    "        print('ERROR!!!')\n",
    "        print(bio_tag)\n",
    "    \n",
    "    pos_bio = []\n",
    "    \n",
    "    for i in range(len(lst_final)):\n",
    "        if 'No' in lst_final[i]:\n",
    "            pos_bio.append('O')\n",
    "        else:\n",
    "            rep = bio_tag_split[i][:2]+lst_final[i]\n",
    "            pos_bio.append(rep)\n",
    "    \n",
    "    pos_bio_line = ' '.join(pos_bio)\n",
    "    \n",
    "    return pos_bio_line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a339542",
   "metadata": {},
   "source": [
    "### PO-TreeCRFs(CP) Formatting With POS in biotag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200372d7",
   "metadata": {},
   "source": [
    "#### With Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc347a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file .\\Data\\Model-3 with POS with Context already exists.\n"
     ]
    }
   ],
   "source": [
    "!mkdir \".\\Data\\Model-3 with POS with Context\"\n",
    "!mkdir \".\\Data\\Model-3 with POS without Context\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f599cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd \"./With Context CSV files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d499faa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "csv_files= ['./train.csv','./dev.csv','./test.csv','./outofDomain.csv']\n",
    "for j in range(2): #For coarse and finegrain data. If there is only one type of format, just modify it to range(1)\n",
    "    for csv in csv_files:\n",
    "        df= pd.read_csv(csv)\n",
    "        sentences= df['Clean']\n",
    "        bio_tag= df['Bio_tagged']\n",
    "        if j == 0:\n",
    "            output_file= '../Data/Model-3 with POS with Context/Coarse/'+csv[2:-3]+'data'\n",
    "            tag_span= df['Coarse_Span_Tagged']\n",
    "        elif j == 1:\n",
    "            output_file= '../Data/Model-3 with POS with Context/Finegrain/'+csv[2:-3]+'data'\n",
    "            tag_span= df['Span_Tagged']\n",
    "        with open(output_file,'w') as f:\n",
    "            for i in range(df.shape[0]):\n",
    "                f.write(sentences[i].strip()+'\\n')\n",
    "                f.write(POS_BIO(bio_tag[i])+'\\n')\n",
    "#                 tag_span[i]= tag_span[i].strip('|')\n",
    "                f.write(tag_span[i].strip()+'\\n')\n",
    "                f.write('\\n')\n",
    "        print(csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd83628",
   "metadata": {},
   "source": [
    "#### No context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31db1c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd \"../No Context CSV files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292b9516",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files= ['./train.csv','./dev.csv','./test.csv','./outofDomain.csv']\n",
    "for j in range(2):\n",
    "    for csv in csv_files:\n",
    "        df= pd.read_csv(csv)\n",
    "        sentences= df['Clean']\n",
    "        bio_tag= df['Bio_tagged']\n",
    "        if j == 0:\n",
    "            output_file= '../Data/Model-3 with POS without Context/Coarse/'+csv[2:-3]+'data'\n",
    "            tag_span= df['Coarse_Span_Tagged']\n",
    "        elif j == 1:\n",
    "            output_file= '../Data/Model-3 with POS without Context/Finegrain/'+csv[2:-3]+'data'\n",
    "            tag_span= df['Span_Tagged']\n",
    "        with open(output_file,'w') as f:\n",
    "            for i in range(df.shape[0]):\n",
    "                f.write(sentences[i].strip()+'\\n')\n",
    "                f.write(POS_BIO(bio_tag[i])+'\\n')\n",
    "#                 tag_span[i]= tag_span[i].strip('|')\n",
    "                f.write(tag_span[i].strip()+'\\n')\n",
    "                f.write('\\n')\n",
    "        print(csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5c50bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd \"../\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78111bb",
   "metadata": {},
   "source": [
    "### nner_as_parsing(LexCP) POS in clean line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c694bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir \".\\Data\\Model-4 with POS with Context\"\n",
    "!mkdir '.\\Data\\Model-4 with POS without Context'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f7de79",
   "metadata": {},
   "source": [
    "#### With Context files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd8d1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd \"With Context CSV files\"\n",
    "\n",
    "csv_files= ['./train.csv','./dev.csv','./test.csv','./outofDomain.csv']\n",
    "\n",
    "for k in range(2):\n",
    "    for csv in csv_files:\n",
    "        df= pd.read_csv(csv)\n",
    "        sentences= df['Clean']\n",
    "        biotag_lines = df['Bio_tagged']\n",
    "        if k == 0:\n",
    "            tag_span= df['Coarse_Span_Tagged']     \n",
    "            output_file= '../Data/Model-4 with POS with Context/Coarse/'+csv[2:-3]+'data'\n",
    "        elif k == 1:\n",
    "            tag_span= df['Span_Tagged']     \n",
    "            output_file= '../Data/Model-4 with POS with Context/Finegrain/'+csv[2:-3]+'data'\n",
    "\n",
    "        with open(output_file,'w') as f:\n",
    "            for i in range(df.shape[0]):\n",
    "                pos_clean_line = Clean_POS(sentences[i],biotag_lines[i])\n",
    "                f.write(pos_clean_line+'\\n')\n",
    "#                 tag_span[i]= tag_span[i].strip('|')\n",
    "                f.write(tag_span[i]+'\\n')\n",
    "                f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dde1b97",
   "metadata": {},
   "source": [
    "#### No Context files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f9f99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd \"../No Context CSV files\"\n",
    "\n",
    "csv_files= ['./train.csv','./dev.csv','./test.csv','./outofDomain.csv']\n",
    "\n",
    "for k in range(2):\n",
    "    for csv in csv_files:\n",
    "        df= pd.read_csv(csv)\n",
    "        sentences= df['Clean']\n",
    "        biotag_lines = df['Bio_tagged']\n",
    "        if k == 0:\n",
    "            tag_span= df['Coarse_Span_Tagged']     \n",
    "            output_file= '../Data/Model-4 with POS without Context/Coarse/'+csv[2:-3]+'data'\n",
    "        elif k == 1:\n",
    "            tag_span= df['Span_Tagged']     \n",
    "            output_file= '../Data/Model-4 with POS without Context/Finegrain/'+csv[2:-3]+'data'\n",
    "\n",
    "        with open(output_file,'w') as f:\n",
    "            for i in range(df.shape[0]):\n",
    "                pos_clean_line = Clean_POS(sentences[i],biotag_lines[i])\n",
    "                f.write(pos_clean_line+'\\n')\n",
    "#                 tag_span[i]= tag_span[i].strip('|')\n",
    "                f.write(tag_span[i]+'\\n')\n",
    "                f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f8ceb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd \"../\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b4c2c5",
   "metadata": {},
   "source": [
    "### Tri-affine(SpanCL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6da1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir \".\\Data\\Model-5 with POS with Context\"\n",
    "!mkdir '.\\Data\\Model-5 with POS without Context'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02199ad2",
   "metadata": {},
   "source": [
    "#### Function for POS for SpanCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62972ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def POS(bio_tag):\n",
    "    \n",
    "    bio_tag_split = bio_tag.strip().split()\n",
    "    lst = []\n",
    "    for i in range(len(bio_tag_split)):\n",
    "        bio = bio_tag_split[i]\n",
    "        if bio == 'B-C':\n",
    "            if bio_tag_split[i-1] == 'I-C'and i!=0:\n",
    "                lst.append(z+1)\n",
    "            z = 0\n",
    "        elif bio == 'I-C':\n",
    "            z += 1\n",
    "        elif bio == 'O':\n",
    "            if bio_tag_split[i-1] == 'I-C' and i!=0:\n",
    "                lst.append(z+1)\n",
    "                lst.append(1)\n",
    "            else:\n",
    "                lst.append(1)\n",
    "    if bio_tag_split[-1] == 'I-C':\n",
    "        lst.append(z+1)\n",
    "    lst_final = []\n",
    "    for b in lst:\n",
    "        for x in range(b):\n",
    "            lst_final.append(b)\n",
    "    \n",
    "    lst_final = ['No' if key==1 else key for key in lst_final]\n",
    "    lst_final = ['Comp'+str(key) for key in lst_final]\n",
    "    \n",
    "    return lst_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce64779e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_span_to_ner(string):\n",
    "    lst= string.split()\n",
    "    if lst!= []:\n",
    "#         print('list is',lst)\n",
    "        reg= '^(\\d+),'\n",
    "        reg2= '(\\d+)$'\n",
    "        reg3= '([^d]\\w+)'\n",
    "        out= ['']*3\n",
    "        out[0]= int(re.findall(reg,lst[0])[0])\n",
    "        out[1]= int(re.findall(reg2,lst[0])[0])\n",
    "        out[2]= lst[1] \n",
    "    else:\n",
    "        out= []\n",
    "    return out\n",
    "\n",
    "def tag_to_entities(tag_span):\n",
    "    entity= dict()\n",
    "    entity['start']= tag_span[0]\n",
    "    entity['end']= tag_span[1]\n",
    "    entity['type']= tag_span[-1]\n",
    "    \n",
    "    \n",
    "    return entity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612213ca",
   "metadata": {},
   "source": [
    "#### With Context files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae24122",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd \"With Context CSV files\"\n",
    "\n",
    "csv_files= ['./train.csv','./dev.csv','./test.csv','./outofDomain.csv']\n",
    "for k in range(2):\n",
    "    for csv in csv_files:\n",
    "        ners= []\n",
    "        df= pd.read_csv(csv)\n",
    "        raw_lines = df['Raw_Tagged']\n",
    "        sentences= df['Clean']\n",
    "        bio_tag= df['Bio_tagged']\n",
    "        if k == 0:\n",
    "            tag_span= df['Coarse_Span_Tagged']     \n",
    "            output_file= '../Data/Model-5 with POS with Context/Coarse/'+csv[2:-3]+'json'\n",
    "        elif k == 1:\n",
    "            tag_span= df['Span_Tagged']     \n",
    "            output_file= '../Data/Model-5 with POS with Context/Finegrain/'+csv[2:-3]+'json'\n",
    "        j=0\n",
    "        for i in range(df.shape[0]-df.shape[0]%6):\n",
    "            print(i)\n",
    "#             print(raw_lines[i])\n",
    "#             print(bio_tag[i])\n",
    "            if (i%1 == 0):\n",
    "                j+=1\n",
    "            tokens= sentences[i].split()\n",
    "#             tag_span[i]= tag_span[i].strip('|')\n",
    "            tag_span_list= tag_span[i].split('|')\n",
    "            ner_list= [tag_span_to_ner(span) for span in tag_span_list]\n",
    "            entities= [tag_to_entities(y) for y in ner_list if y!=[]] if tag_span_list!=[''] else []\n",
    "            new_dict= dict()\n",
    "            new_dict['tokens']=tokens\n",
    "            new_dict['entities']= entities\n",
    "            new_dict['relations']= dict()\n",
    "            x= \"{:04d}\".format(j)\n",
    "            if csv == csv_files[3]:\n",
    "                new_dict['org_id']= \"ge/test/\"+str(x)\n",
    "            else:\n",
    "                new_dict['org_id']= \"ge/\"+str(csv[2:-4])+\"/\"+str(x)\n",
    "            new_dict['pos']= POS(bio_tag[i])\n",
    "            new_dict['ltokens']= []\n",
    "            new_dict['rtokens']= []\n",
    "            ners.append(new_dict)\n",
    "            tokens= []\n",
    "            entities= []\n",
    "        with open(output_file,'w') as f:\n",
    "            json.dump(ners,f,indent= 2)        \n",
    "\n",
    "        print('Data formatting done for ', csv[2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb99675",
   "metadata": {},
   "source": [
    "#### No Context files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4609f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd \"../No Context CSV files\"\n",
    "\n",
    "csv_files= ['./train.csv','./dev.csv','./test.csv','./outofDomain.csv']\n",
    "for k in range(2):\n",
    "    for csv in csv_files:\n",
    "        ners= []\n",
    "        df= pd.read_csv(csv)\n",
    "        raw_lines = df['Raw_Tagged']\n",
    "        sentences= df['Clean']\n",
    "        bio_tag= df['Bio_tagged']\n",
    "        if k == 0:\n",
    "            tag_span= df['Coarse_Span_Tagged']     \n",
    "            output_file= '../Data/Model-5 with POS without Context/Coarse/'+csv[2:-3]+'json'\n",
    "        elif k == 1:\n",
    "            tag_span= df['Span_Tagged']     \n",
    "            output_file= '../Data/Model-5 with POS without Context/Finegrain/'+csv[2:-3]+'json'\n",
    "        j=0\n",
    "        for i in range(df.shape[0]-df.shape[0]%6):\n",
    "            print(i)\n",
    "#             print(raw_lines[i])\n",
    "#             print(bio_tag[i])\n",
    "            if (i%1 == 0):\n",
    "                j+=1\n",
    "            tokens= sentences[i].split()\n",
    "#             tag_span[i]= tag_span[i].strip('|')\n",
    "            tag_span_list= tag_span[i].split('|')\n",
    "            ner_list= [tag_span_to_ner(span) for span in tag_span_list]\n",
    "            entities= [tag_to_entities(y) for y in ner_list if y!=[]] if tag_span_list!=[''] else []\n",
    "            new_dict= dict()\n",
    "            new_dict['tokens']=tokens\n",
    "            new_dict['entities']= entities\n",
    "            new_dict['relations']= dict()\n",
    "            x= \"{:04d}\".format(j)\n",
    "            if csv == csv_files[3]:\n",
    "                new_dict['org_id']= \"ge/test/\"+str(x)\n",
    "            else:\n",
    "                new_dict['org_id']= \"ge/\"+str(csv[2:-4])+\"/\"+str(x)\n",
    "            new_dict['pos']= POS(bio_tag[i])\n",
    "            new_dict['ltokens']= []\n",
    "            new_dict['rtokens']= []\n",
    "            ners.append(new_dict)\n",
    "            tokens= []\n",
    "            entities= []\n",
    "        with open(output_file,'w') as f:\n",
    "            json.dump(ners,f,indent= 2)        \n",
    "\n",
    "        print('Data formatting done for ', csv[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df79136",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd \"../\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66c34bf",
   "metadata": {},
   "source": [
    "### For pointer-net(BotCP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0713841",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir \".\\Data\\Model-6 with POS with Context\"\n",
    "!mkdir '.\\Data\\Model-6 with POS without Context'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db8b5db",
   "metadata": {},
   "source": [
    "#### With Context files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640a13f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd \"With Context CSV files\"\n",
    "\n",
    "csv_files= ['./train.csv','./dev.csv','./test.csv','./outofDomain.csv']\n",
    "for k in range(2):\n",
    "    for csv in csv_files:\n",
    "        df= pd.read_csv(csv)\n",
    "        sentences= df['Clean']\n",
    "        biotag_lines = df['Bio_tagged']\n",
    "        if k == 0:\n",
    "            tag_span= df['Coarse_Span_Tagged']     \n",
    "            output_file= '../Data/Model-6 with POS with Context/Coarse/'+'./genia.'+csv[2:-3]\n",
    "        elif k == 1:\n",
    "            tag_span= df['Span_Tagged']     \n",
    "            output_file= '../Data/Model-6 with POS with Context/Finegrain/'+'./genia.'+csv[2:-3]\n",
    "        if 'outofDomain' in output_file:\n",
    "            output_file = re.sub('outofDomain','test',output_file)\n",
    "            output_file = re.sub('genia','ood_genia',output_file)\n",
    "        print(output_file)\n",
    "        with open(output_file,'w') as f:\n",
    "            for i in range(df.shape[0]):\n",
    "                pos_clean_line = Clean_POS(sentences[i],biotag_lines[i])\n",
    "                f.write(pos_clean_line+'\\n')\n",
    "#                 tag_span[i]= tag_span[i].strip('|')\n",
    "                f.write(tag_span[i]+'\\n')\n",
    "                f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba87e676",
   "metadata": {},
   "source": [
    "#### No Context files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6321a147",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd \"../No Context CSV files\"\n",
    "\n",
    "csv_files= ['./train.csv','./dev.csv','./test.csv','./outofDomain.csv']\n",
    "for k in range(2):\n",
    "    for csv in csv_files:\n",
    "        df= pd.read_csv(csv)\n",
    "        sentences= df['Clean']\n",
    "        biotag_lines = df['Bio_tagged']\n",
    "        if k == 0:\n",
    "            tag_span= df['Coarse_Span_Tagged']     \n",
    "            output_file= '../Data/Model-6 with POS without Context/Coarse/'+'./genia.'+csv[2:-3]\n",
    "        elif k == 1:\n",
    "            tag_span= df['Span_Tagged']     \n",
    "            output_file= '../Data/Model-6 with POS without Context/Finegrain/'+'./genia.'+csv[2:-3]\n",
    "        if 'outofDomain' in output_file:\n",
    "            output_file = re.sub('outofDomain','test',output_file)\n",
    "            output_file = re.sub('genia','ood_genia',output_file)\n",
    "        print(output_file)\n",
    "        with open(output_file,'w') as f:\n",
    "            for i in range(df.shape[0]):\n",
    "                pos_clean_line = Clean_POS(sentences[i],biotag_lines[i])\n",
    "                f.write(pos_clean_line+'\\n')\n",
    "#                 tag_span[i]= tag_span[i].strip('|')\n",
    "                f.write(tag_span[i]+'\\n')\n",
    "                f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6d398a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd \"../\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea51fab",
   "metadata": {},
   "source": [
    "### For BARTNER(Seq2Seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f586b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_span_to_ner(string):\n",
    "    lst= string.split()\n",
    "    if lst!= []:\n",
    "#         print('list is',lst)\n",
    "        reg= '^(\\d+),'\n",
    "        reg2= '(\\d+)$'\n",
    "        reg3= '([^d]\\w+)'\n",
    "        out= ['']*3\n",
    "        out[0]= int(re.findall(reg,lst[0])[0])\n",
    "        out[1]= int(re.findall(reg2,lst[0])[0])-1\n",
    "        out[2]= lst[1] \n",
    "    else:\n",
    "        out= []\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a012ed4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir \".\\Data\\Model-7 with POS with Context\"\n",
    "!mkdir '.\\Data\\Model-7 with POS without Context'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ed9745",
   "metadata": {},
   "source": [
    "#### With Context files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2856f22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd \"With Context CSV files\"\n",
    "\n",
    "csv_files= ['./train.csv','./dev.csv','./test.csv','./outofDomain.csv']\n",
    "for k in range(2):\n",
    "    for csv in csv_files:\n",
    "        df= pd.read_csv(csv)\n",
    "        sentences= df['Clean']\n",
    "        biotag_lines = df['Bio_tagged']\n",
    "        \n",
    "        if k == 0:\n",
    "            tag_span= df['Coarse_Span_Tagged']     \n",
    "            jsl_file= '../Data/Model-7 with POS with Context/Coarse/'+csv[2:-3]+'jsonlines'\n",
    "        elif k == 1:\n",
    "            tag_span= df['Span_Tagged']     \n",
    "            jsl_file= '../Data/Model-7 with POS with Context/Finegrain/'+csv[2:-3]+'jsonlines'\n",
    "        print(len(sentences))\n",
    "        i=0\n",
    "        j=0\n",
    "        l=0\n",
    "        dict_list= []\n",
    "        while i < (df.shape[0]- df.shape[0]%6):\n",
    "            batch_tokens= []\n",
    "            batch_ners= []\n",
    "            count=0\n",
    "            while j<6:\n",
    "#                 tokens= sentences[i].split()\n",
    "                pos_clean_line = Clean_POS(sentences[i],biotag_lines[i])\n",
    "                tokens = pos_clean_line.split()\n",
    "                tag_span[i].strip()\n",
    "#                 tag_span[i]= tag_span[i].strip('|')\n",
    "                tag_span_list= tag_span[i].split('|')\n",
    "                if [] in tag_span_list:\n",
    "                    count+=1\n",
    "                ner_list= [tag_span_to_ner(span) for span in tag_span_list]\n",
    "                if tag_span_list!= []:\n",
    "                    batch_tokens.append(tokens)\n",
    "                    batch_ners.append(ner_list)\n",
    "                tokens= []\n",
    "                i+=1\n",
    "                j+=1\n",
    "            j=0\n",
    "            l+=1\n",
    "            dct= dict()\n",
    "            dct['ners'] = batch_ners\n",
    "            dct['sentences'] = batch_tokens\n",
    "            dict_list.append(dct) \n",
    "        with open('new.json','w') as f:\n",
    "            json.dump(dict_list,f)\n",
    "        file= open(\"new.json\")\n",
    "        JSON_file= json.load(file)\n",
    "        with open(jsl_file, 'w') as outfile:\n",
    "            for entry in JSON_file:\n",
    "                json.dump(entry, outfile)\n",
    "                outfile.write('\\n')\n",
    "\n",
    "        file.close()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3331f44a",
   "metadata": {},
   "source": [
    "#### No Context files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d19508",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd \"../No Context CSV files\"\n",
    "\n",
    "csv_files= ['./train.csv','./dev.csv','./test.csv','./outofDomain.csv']\n",
    "for k in range(2):\n",
    "    for csv in csv_files:\n",
    "        df = pd.read_csv(csv)\n",
    "        sentences = df['Clean']\n",
    "        biotag_lines = df['Bio_tagged']\n",
    "        \n",
    "        if k == 0:\n",
    "            tag_span= df['Coarse_Span_Tagged']     \n",
    "            jsl_file = '../Data/Model-7 with POS without Context/Coarse/'+csv[2:-3]+'jsonlines'\n",
    "        elif k == 1:\n",
    "            tag_span= df['Span_Tagged']     \n",
    "            jsl_file = '../Data/Model-7 with POS without Context/Finegrain/'+csv[2:-3]+'jsonlines'\n",
    "        print(len(sentences))\n",
    "        i=0\n",
    "        j=0\n",
    "        l=0\n",
    "        dict_list = []\n",
    "        while i < (df.shape[0]- df.shape[0]%6):\n",
    "            batch_tokens = []\n",
    "            batch_ners = []\n",
    "            count = 0\n",
    "            while j<6:\n",
    "#                 tokens= sentences[i].split()\n",
    "                pos_clean_line = Clean_POS(sentences[i],biotag_lines[i])\n",
    "                tokens = pos_clean_line.split()\n",
    "                tag_span[i].strip()\n",
    "#                 tag_span[i]= tag_span[i].strip('|')\n",
    "                tag_span_list = tag_span[i].split('|')\n",
    "                if [] in tag_span_list:\n",
    "                    count +=1\n",
    "                ner_list= [tag_span_to_ner(span) for span in tag_span_list]\n",
    "                if tag_span_list!= []:\n",
    "                    batch_tokens.append(tokens)\n",
    "                    batch_ners.append(ner_list)\n",
    "                tokens= []\n",
    "                i+=1\n",
    "                j+=1\n",
    "            j=0\n",
    "            l+=1\n",
    "            dct= dict()\n",
    "            dct['ners'] = batch_ners\n",
    "            dct['sentences'] = batch_tokens\n",
    "            dict_list.append(dct) \n",
    "        with open('new.json','w') as f:\n",
    "            json.dump(dict_list,f)\n",
    "        file= open(\"new.json\")\n",
    "        JSON_file= json.load(file)\n",
    "        with open(jsl_file, 'w') as outfile:\n",
    "            for entry in JSON_file:\n",
    "                json.dump(entry, outfile)\n",
    "                outfile.write('\\n')\n",
    "\n",
    "        file.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032d1308",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd \"../\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
